# ğŸ’¬ Let's Talk

**Letâ€™s Talk** is a modular **AI chat application** built using **FastAPI** for backend APIs and **Streamlit** for the frontend interface.
It integrates **Hugging Face models** (via the Inference API) or **Ollama** for local inference and supports **asynchronous streaming** for smooth, real-time chat responses.

---

## ğŸ–¼ï¸ Preview


<p align="center">
  <img src="assets/demo.png" width="600">
</p>

---

## âš™ï¸ Tech Stack

| Layer              | Technology                |
| :----------------- | :------------------------ |
| **Frontend**       | Streamlit                 |
| **Backend**        | FastAPI + Uvicorn         |
| **LLM Providers**  | Hugging Face Hub / Ollama |
| **Async Engine**   | Python `asyncio` + SSE    |  |

---

## ğŸš€ Quickstart

### 1ï¸âƒ£ Setup environment

```bash
python -m venv .venv
source .venv/bin/activate      # On Windows: .venv\Scripts\activate
```

### 2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Configure environment

```bash
cp .env.example .env
# or on Windows
copy .env.example .env
```

Then open `.env` and **add your Hugging Face API token**:

```bash
HFHUB_API_TOKEN=hf_your_token_here
```

ğŸ‘‰ If you donâ€™t have one yet:

1. Go to [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
2. Click **â€œNew Tokenâ€** â†’ choose type **Read**
3. Copy it and paste into your `.env` file as shown above.

---

### 4ï¸âƒ£ Start backend (FastAPI)

```bash
uvicorn app.main:app --reload --port 8000
```

### 5ï¸âƒ£ Start frontend (Streamlit)

In a new terminal:

```bash
streamlit run frontend/app.py --server.port 8501
```

âœ… Open the Streamlit app at [http://localhost:8501](http://localhost:8501)

---

## ğŸ§© Features

* âš¡ **FastAPI backend** for scalable, async inference
* ğŸ’¬ **Streamlit chat interface** with real-time updates
* ğŸ”Œ **Multiple model providers** (Hugging Face, Ollama)
* ğŸ§  **Session memory** for chat continuity
* ğŸ§° Modular design for easy extensions (tools, agents, RAG, etc.)

---

## ğŸ§± Project Structure

```
letstalk/
â”œâ”€ app/                     # FastAPI backend
â”‚  â”œâ”€ main.py
â”‚  â”œâ”€ routers/
â”‚  â”œâ”€ services/
â”‚  â”œâ”€ schemas/
â”‚  â”œâ”€ utils/
â”‚  â””â”€ core/
â”‚     â””â”€ config.py
â”œâ”€ frontend/                # Streamlit frontend
â”‚  â””â”€ app.py
â”œâ”€ assets/                  # Images for README or UI
â”œâ”€ tests/
â”œâ”€ requirements.txt
â””â”€ .env.example
```

---

## âš™ï¸ Environment Variables (`.env`)

```bash
# Default: Hugging Face setup
PROVIDER=hfhub
HFHUB_REPO_ID=mistralai/Mistral-7B-Instruct-v0.2
HFHUB_API_TOKEN=hf_your_token_here
SSE_CHUNK_DELAY_MS=0

# Optional: Ollama setup
PROVIDER=ollama
OLLAMA_MODEL=mistral
OLLAMA_BASE_URL=http://localhost:11434
```

---

## ğŸ§ª Testing

```bash
pytest -v
```

---

## ğŸ“¸ Screenshots

Place your screenshots in `assets/` and embed them like:

```markdown
![Home UI](assets/home.png)
![Chat Demo](assets/chat.png)
```

---

## ğŸ§­ Roadmap

* [ ] Integrate FAISS-based RAG
* [ ] Add authentication
* [ ] Deploy on AWS / Render
* [ ] Add conversation analytics dashboard

---

## ğŸ§‘â€ğŸ’» Author

**Harshvardhan Khanna**
AI Engineer | M.Sc. Computer Science, TU Ilmenau
[LinkedIn](https://www.linkedin.com/in/harshkhanna-ai) â€¢ [GitHub](https://github.com/harshkhanna-ai)

---
